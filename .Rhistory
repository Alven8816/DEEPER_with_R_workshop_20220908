library(pacman)
p_load("tidyverse")
install.packages("tinytex")
library(rlang)
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# load library
library(ggplot2)
library(caret)
library(skimr)
library(CAST)
library(SuperLearner)
library(deeper)
data("envir_example")
knitr::kable(x = head(envir_example[1:6, 1:8]), digits = 2)
set.seed(1234) # to achieve a repeatable results
size <-
caret::createDataPartition(y = envir_example$PM2.5,
p = 0.8, list = FALSE)
trainset <- envir_example[size, ]
testset <- envir_example[-size, ]
y <- c("PM2.5")
x <- colnames(envir_example[-c(1, 6)]) # except "date" and "PM2.5"
ranger <-
tuningModel(
basemodel  = 'SL.ranger',
params = list(num.trees = 100),
tune = list(mtry = c(1, 3, 7))
)
# training the RF model with different parameters simutaniously
model1 <-
predictModel(
Y = trainset[, y],
X = trainset[, x],
base_model = c(ranger),
cvControl = list(V = 5)
)
#print(model1$base_ensemble_value)
# predict the new testing dataset
pred_model1 <- deeper::predict(object = model1, newX = testset[, x])
# get each base model prediction
head(pred_model1$pre_base$library.predict)
## conduct the spatial CV
# Create a list with 7 (folds) elements (each element contains index of rows to be considered on each fold)
unique(trainset$code) # there are 7 stations in the trainset
indices <-
CAST::CreateSpacetimeFolds(trainset, spacevar = "code", k = 7)
# Rows of validation set on each fold
v_raw <- indices$indexOut
names(v_raw) <- seq(1:7)
model2 <- predictModel_parallel(
Y = trainset[, y],
X = trainset[, x],
base_model = c("SL.xgboost", "SL.ranger"),
cvControl = list(V = length(v_raw), validRows = v_raw),
number_cores = 4,
seed = 1234
)
## when number_cores is missing, it will indicate user to set one based on the operation system.
# pred_m3 <- predictModel_parallel(
#     Y = trainset[,y],
#     X = trainset[,x],
#     base_model = c("SL.xgboost",ranger),
#     cvControl = list(V = length(v_raw), validRows = v_raw),
#     seed = 1
#   )
#You have 8 cpu cores, How many cpu core you want to use:
# type the number to continue the process.
# prediction
pred_model2 <- deeper::predict(object = model2, newX = testset[, x])
model3_stack <-
stack_ensemble(
object = model1,
meta_model = c("SL.ranger", "SL.xgboost", "SL.glm"),
original_feature = FALSE,
X = trainset[, x]
)
model3_stack$stack_ensemble_value
model3_stack$original_feature
model3_stack$R2
model3_stack$base_model
print(apply(
X = model3_stack$model_summary$SL.predict,
MARGIN = 2,
FUN = caret::R2,
obs = testset[, y]
))
str(model3_stack$model_summary$SL.predict)
pred_model1 <- deeper::predict(object = model1, newX = testset[, x])
head(pred_model1$pre_base$library.predict)
print(apply(
X = pred_model1$pre_base$library.predict,
MARGIN = 2,
FUN = caret::R2,
obs = testset[, y]
))
model2$base_ensemble_value
model3_stack$stack_ensemble_value
model3_DEML <-
deeper::predict(object = model3_stack, newX = testset[, x])
View(model3_DEML)
print(apply(
X = c(model3_DEML$pre_meta$library.predict,
model3_DEML$pre_meta$pred),
MARGIN = 2,
FUN = caret::R2,
obs = testset[, y]
))
print(apply(
X = c(model3_DEML$pre_meta$library.predict,
model3_DEML$pre_meta$pred),
MARGIN = 2,
FUN = caret::R2,
obs = testset[, y]
))
print(apply(
X = cbind(model3_DEML$pre_meta$library.predict,
model3_DEML$pre_meta$pred),
MARGIN = 2,
FUN = caret::R2,
obs = testset[, y]
))
model3_stack$stack_ensemble_value
print(apply(
X = cbind(model3_DEML$pre_meta$library.predict,
model3_DEML$pre_meta$pred),
MARGIN = 2,
FUN = caret::RMSE,
obs = testset[, y]
))
load(
"C:/Users/yuwen/OneDrive - Monash University/2021 work/workshops/20220908 DEEPER workshop/talk materials/quiz/data_Sydney.rda"
)
# dataset split
set.seed(1234)
size <-
caret::createDataPartition(y = data_test$no2_annual,
p = 0.9,
list = FALSE)
trainset_syd <- data_test[size,]
testset_syd <- data_test[-size,]
### Identify the dependence and independence variables
dependence <- c("no2_annual")
independence <- colnames(data_test[-c(1)]) # except
start_time <- Sys.time()
ranger_mtry_7 <-
tuningModel(basemodel  = 'SL.ranger',
params = list(mtry = 7))
model_challenge2 <- predictModel(
Y = trainset_syd[, dependence],
X = trainset_syd[, independence],
base_model = c(ranger_mtry_7, "SL.xgboost"),
cvControl = list(V = 5)
)
end_time <- Sys.time()
end_time - start_time
model_challenging3 <- stack_ensemble(
object = model_challenge2,
Y = trainset_syd[, dependence],
X = trainset_syd[, independence],
meta_model = c("SL.ranger", "SL.xgboost"),
original_feature = TRUE
)
pred_DEML <-
deeper::predict(object = model_challenging3, newX = testset_syd[, independence])
print(apply(
X = pred_DEML$pre_meta$pred,
MARGIN = 2,
FUN = caret::R2,
obs = testset_syd[, dependence]
))
# model performance for test dataset
pred_DEML <-
deeper::predict(object = model_challenging3, newX = testset_syd[, independence])
print(apply(
X = pred_DEML$pre_meta$pred,
MARGIN = 2,
FUN = caret::R2,
obs = testset_syd[, dependence]
))
# load the grid cell dataset
predictor_no2_gsyd_10km_2005_2018 <-
readRDS(
"C:/Users/yuwen/OneDrive - Monash University/2021 work/workshops/20220908 DEEPER workshop/talk materials/quiz/DEEPER_workshop_2022/DEEPER_workshop_2022/predictor_no2_gsyd_10km_2005_2018_WY2.rds"
)
pred_DEML_grid <-
deeper::predict(object = model_challenging3, newX = predictor_no2_gsyd_10km_2005_2018)
# final DEML prediction
head(pred_DEML_grid$pre_meta$pred)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(deeper)
data(model_list)
View(model_details)
View(model_details)
data(model_list) # to get more details about the algorithms
knitr::kable(x = model_details)
library(kableExtra)
library(knitr)
library(rmarkdown)
library(deeper)
data(model_list) # to get more details about the algorithms
#knitr::kable(x = head(model_details))
kbl(cbind(model_details)) %>%
kable_paper() %>%
scroll_box(width = "100%", height = "200px")
library(deeper)
View(model_details)
library(rmarkdown)
render(input = "index.Rmd",output_file = "index.md")
