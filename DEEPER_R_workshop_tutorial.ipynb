{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alven8816/DEEPER_with_R_workshop_20220908/blob/main/DEEPER_R_workshop_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koFk79fbzLdV"
      },
      "source": [
        "# **Deep ensemble machine learning (DEML) for estimating environmental exposure - Session 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WodnimKozUAE"
      },
      "source": [
        "Wenhua(Alven) Yu; Liam Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CZJeLuV1E7t"
      },
      "source": [
        "2022-09-08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBI-8c86Px3-"
      },
      "outputs": [],
      "source": [
        "# check the package installation\n",
        "install.packages(\"pacman\")\n",
        "library(pacman)\n",
        "p_load(\"devtools\",\"SuperLearner\",\"ranger\",\"CAST\",\"caret\",\"skimr\",\"gbm\",\"xgboost\",\"hexbin\")\n",
        "# about 9 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rhKVRO9IfWW"
      },
      "source": [
        "# R package 'deeper' install\n",
        "\n",
        "Before installation, please make sure:\n",
        "\n",
        "-   using R (\\>= 3.5.0)\n",
        "\n",
        "-   installed certain dependent R packages: devtools, SuperLearner(\\>= 2.0-28)\n",
        "\n",
        "-   installed other suggested R packages: caret, skimr, CAST, ranger, gbm, xgboost, (nnls, foreach,splines, gam)\n",
        "\n",
        "\n",
        "The developing version of deeper can be found from [github](https://github.com/Alven8816/deeper).\n",
        "\n",
        "Using the following syntax to install:\n",
        "\n",
        "``` {.r}\n",
        "library(devtools)\n",
        "install_github(\"Alven8816/deeper\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4gjhuSHGqDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02889855-567b-4320-d957-0460eb6c304e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: usethis\n",
            "\n"
          ]
        }
      ],
      "source": [
        "library(devtools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQlWP2apGwHs"
      },
      "outputs": [],
      "source": [
        "install_github(\"Alven8816/deeper\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzs3dQUBHCt5"
      },
      "outputs": [],
      "source": [
        "library(deeper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsUPw0HKOcZF"
      },
      "source": [
        "# Activity\n",
        "\n",
        "Getting access to the Google Colab or install deeper R package in your local computer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms selection"
      ],
      "metadata": {
        "id": "rQ0SdNxLFCPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data(model_list)\n",
        "print(model_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygbfVo26FHEj",
        "outputId": "aa5da1b4-c862-43be-fa22-b632d1121baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m# A tibble: 35 × 4\u001b[39m\n",
            "   parameter          algorithm                                  require…¹ types\n",
            "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                                     \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m\n",
            "\u001b[90m 1\u001b[39m SL.bayesglm        Bayesian generalized linear regression    arm        R    \n",
            "\u001b[90m 2\u001b[39m SL.biglasso        Extending Lasso Model Fitting to Big Data biglasso   R    \n",
            "\u001b[90m 3\u001b[39m SL.caret           random Forest as default                  caret      R    \n",
            "\u001b[90m 4\u001b[39m SL.caret.rpart     decision trees as default                 caret      R    \n",
            "\u001b[90m 5\u001b[39m SL.cforest         Breiman's random forests                  party      R    \n",
            "\u001b[90m 6\u001b[39m SL.earth           Multivariate Adaptive Regression Splines  earth      R    \n",
            "\u001b[90m 7\u001b[39m SL.gam             generalized additive models               gam        N    \n",
            "\u001b[90m 8\u001b[39m SL.gbm             generalized boosting algorithm            gbm        R    \n",
            "\u001b[90m 9\u001b[39m SL.glm             generalized linear models                 \u001b[31mNA\u001b[39m         R    \n",
            "\u001b[90m10\u001b[39m SL.glm.interaction generalized linear models                 \u001b[31mNA\u001b[39m         R    \n",
            "\u001b[90m# … with 25 more rows, and abbreviated variable name ¹​` required package`\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the 'type' column, \"R\": can be used for regression or classification;\"N\": can be used for regression but variables require to be numeric style; \"C\": just used in classification.\n"
      ],
      "metadata": {
        "id": "su2VO8AYFMqm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY1PgJ_qRGq5"
      },
      "source": [
        "# Basic steps for DEML\n",
        "\n",
        "* **Step 1.  Data preparation**\n",
        "\n",
        "Including data collection, data clean, setting trianing and testing dataset, and independent variables selection.\n",
        "\n",
        "**Note: DEML could not directly deal with missing values and missing value imputation technologies is recommended prior to the use of the DEML model.**\n",
        "\n",
        "    \n",
        "* **Step 2.  Establish base models**\n",
        "\n",
        "Using predictModel() or predictModel_parallel() to establish the base models. A tuningModel() function can be used to tuning the parameters to get the best single base model.\n",
        "\n",
        "* **Step 3.  Stacking meta models**\n",
        "\n",
        "We use stack_ensemble(),stack_ensemble.fit(), or stack_ensemble_parallel() function to stack the meta models to get a DEML model.\n",
        "\n",
        "\n",
        "* **Step 4.  Prediction based on new data set**\n",
        "\n",
        "After establishment of DEML model, the predict() can be used predict the unseen data set.\n",
        "\n",
        "To assess the performance of the models, assess.plot() can be used by comparing the original observations (y in test set) and the prediction. The assess.plot() also return the point scatter plot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOAMwAw2LLyf"
      },
      "source": [
        "# Example\n",
        "\n",
        "*To estimate the daily ambient PM2.5 in the northeast of China in 2015-2016.*\n",
        "\n",
        "## 1.Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "XCNVAhwMNeVG",
        "outputId": "a67d971b-4505-4b0a-b571-c2770ceaa3cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "|date      |code  |year |month |week |     PM2.5|     c_AOD|      TEMP|        AP|       RH| elevation|        WS| a_buffer10|\n",
              "|:---------|:-----|:----|:-----|:----|---------:|---------:|---------:|---------:|--------:|---------:|---------:|----------:|\n",
              "|2/01/2015 |1001A |2015 |1     |5    |  51.26087| 0.2191133| -2.757819| 1012.1792| 30.50154|        45| 1.6981146|         68|\n",
              "|3/01/2015 |1001A |2015 |1     |6    | 154.04167| 0.9245346| -3.757023| 1000.7333| 43.59143|        45| 0.9536832|         68|\n",
              "|4/01/2015 |1001A |2015 |1     |0    | 151.91667| 0.3927248| -1.298150|  999.6592| 43.22169|        45| 1.3580554|         68|\n",
              "|6/01/2015 |1001A |2015 |1     |2    |  39.54545| 0.3009302| -2.218822| 1010.8966| 20.39437|        45| 2.7473587|         68|\n",
              "|8/01/2015 |1001A |2015 |1     |4    | 148.38095| 1.0532465| -3.522646| 1012.8126| 54.77821|        45| 1.1803217|         68|\n",
              "|9/01/2015 |1001A |2015 |1     |5    |  87.78261| 0.1762094| -1.593402| 1011.9749| 37.70417|        45| 2.1804410|         68|"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "data(\"envir_example\")\n",
        "\n",
        "knitr::kable(head(envir_example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhOkPdIVNtQW"
      },
      "source": [
        "## 1.1 Data clean\n",
        "\n",
        "The basic data clean strategies include:\n",
        "\n",
        "1.  Variable type setting\n",
        "\n",
        "2.  Extreme value (outliers) detection\n",
        "\n",
        "3.  Missing value operation (imputation, drop)\n",
        "\n",
        "4.  Data transforming (normalization/standardization, eg. scale, centralize,log-transform and others)\n",
        "\n",
        "``` {.r}\n",
        "# skim the data missing value and distribmution\n",
        "skimr::skim(envir_example)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4lM6IjxeSogr",
        "outputId": "f6b5053f-9cf1-4a2c-d0d7-f89657b4a7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "── Data Summary ────────────────────────\n",
            "                           Values       \n",
            "Name                       envir_example\n",
            "Number of rows             2970         \n",
            "Number of columns          13           \n",
            "_______________________                 \n",
            "Column type frequency:                  \n",
            "  character                1            \n",
            "  factor                   4            \n",
            "  numeric                  5            \n",
            "________________________                \n",
            "Group variables            None         \n",
            "\n",
            "── Variable type: character ────────────────────────────────────────────────────\n",
            "  skim_variable n_missing complete_rate min max empty n_unique whitespace\n",
            "\u001b[90m1\u001b[39m date                  0             1   9  10     0      501          0\n",
            "\n",
            "── Variable type: factor ───────────────────────────────────────────────────────\n",
            "  skim_variable n_missing complete_rate ordered n_unique\n",
            "\u001b[90m1\u001b[39m code                  0             1 FALSE          7\n",
            "\u001b[90m2\u001b[39m year                  0             1 FALSE          2\n",
            "\u001b[90m3\u001b[39m month                 0             1 FALSE         12\n",
            "\u001b[90m4\u001b[39m week                  0             1 FALSE          7\n",
            "  top_counts                            \n",
            "\u001b[90m1\u001b[39m 100: 436, 100: 432, 100: 427, 100: 427\n",
            "\u001b[90m2\u001b[39m 201: 1511, 201: 1459                  \n",
            "\u001b[90m3\u001b[39m 3: 316, 1: 309, 4: 289, 12: 279       \n",
            "\u001b[90m4\u001b[39m 2: 445, 4: 439, 1: 435, 5: 430        \n",
            "\n",
            "── Variable type: numeric ──────────────────────────────────────────────────────\n",
            "  skim_variable n_missing complete_rate    mean     sd       p0     p25     p50\n",
            "\u001b[90m1\u001b[39m PM2.5                 0             1  70.9   65.2     4.62    23.9    52.3  \n",
            "\u001b[90m2\u001b[39m c_AOD                 0             1   0.674  0.673   0.021\u001b[4m2\u001b[24m   0.219   0.412\n",
            "\u001b[90m3\u001b[39m TEMP                  0             1  11.3   11.3   -\u001b[31m12\u001b[39m\u001b[31m.\u001b[39m\u001b[31m1\u001b[39m      0.114  12.7  \n",
            "\u001b[90m4\u001b[39m AP                    0             1 995.    18.3   939.     989.    998.   \n",
            "\u001b[90m5\u001b[39m RH                    0             1  50.6   17.3     9.73    37.1    50.2  \n",
            "       p75    p100 hist \n",
            "\u001b[90m1\u001b[39m   97.9    450    ▇▂▁▁▁\n",
            "\u001b[90m2\u001b[39m    0.887    3.56 ▇▂▁▁▁\n",
            "\u001b[90m3\u001b[39m   21.9     31.5  ▃▇▅▇▆\n",
            "\u001b[90m4\u001b[39m \u001b[4m1\u001b[24m008.    \u001b[4m1\u001b[24m029.   ▁▁▃▇▃\n",
            "\u001b[90m5\u001b[39m   65.0     88.8  ▂▇▇▇▃\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<dl>\n",
              "\t<dt>$character</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A one_skim_df: 1 × 8</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>skim_variable</th><th scope=col>n_missing</th><th scope=col>complete_rate</th><th scope=col>min</th><th scope=col>max</th><th scope=col>empty</th><th scope=col>n_unique</th><th scope=col>whitespace</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>date</td><td>0</td><td>1</td><td>9</td><td>10</td><td>0</td><td>501</td><td>0</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$factor</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A one_skim_df: 4 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>skim_variable</th><th scope=col>n_missing</th><th scope=col>complete_rate</th><th scope=col>ordered</th><th scope=col>n_unique</th><th scope=col>top_counts</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>code </td><td>0</td><td>1</td><td>FALSE</td><td> 7</td><td>100: 436, 100: 432, 100: 427, 100: 427</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>year </td><td>0</td><td>1</td><td>FALSE</td><td> 2</td><td>201: 1511, 201: 1459                  </td></tr>\n",
              "\t<tr><th scope=row>3</th><td>month</td><td>0</td><td>1</td><td>FALSE</td><td>12</td><td>3: 316, 1: 309, 4: 289, 12: 279       </td></tr>\n",
              "\t<tr><th scope=row>4</th><td>week </td><td>0</td><td>1</td><td>FALSE</td><td> 7</td><td>2: 445, 4: 439, 1: 435, 5: 430        </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$numeric</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A one_skim_df: 5 × 11</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>skim_variable</th><th scope=col>n_missing</th><th scope=col>complete_rate</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>p0</th><th scope=col>p25</th><th scope=col>p50</th><th scope=col>p75</th><th scope=col>p100</th><th scope=col>hist</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>PM2.5</td><td>0</td><td>1</td><td> 70.9060220</td><td>65.2264536</td><td>  4.62500000</td><td> 23.9154125</td><td> 52.3405800</td><td>  97.9219350</td><td> 450.000000</td><td>▇▂▁▁▁</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>c_AOD</td><td>0</td><td>1</td><td>  0.6743631</td><td> 0.6727058</td><td>  0.02121083</td><td>  0.2191133</td><td>  0.4121369</td><td>   0.8873678</td><td>   3.562631</td><td>▇▂▁▁▁</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>TEMP </td><td>0</td><td>1</td><td> 11.2723604</td><td>11.3385907</td><td>-12.11607000</td><td>  0.1136484</td><td> 12.7025250</td><td>  21.9393125</td><td>  31.457240</td><td>▃▇▅▇▆</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>AP   </td><td>0</td><td>1</td><td>995.0750959</td><td>18.2763409</td><td>939.35136800</td><td>989.3052500</td><td>998.3992768</td><td>1007.6977500</td><td>1028.826000</td><td>▁▁▃▇▃</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>RH   </td><td>0</td><td>1</td><td> 50.6296481</td><td>17.2962454</td><td>  9.72653456</td><td> 37.0803125</td><td> 50.1837096</td><td>  64.9926675</td><td>  88.755170</td><td>▂▇▇▇▃</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "</dl>\n"
            ],
            "text/markdown": "$character\n:   \nA one_skim_df: 1 × 8\n\n| <!--/--> | skim_variable &lt;chr&gt; | n_missing &lt;int&gt; | complete_rate &lt;dbl&gt; | min &lt;int&gt; | max &lt;int&gt; | empty &lt;int&gt; | n_unique &lt;int&gt; | whitespace &lt;int&gt; |\n|---|---|---|---|---|---|---|---|---|\n| 1 | date | 0 | 1 | 9 | 10 | 0 | 501 | 0 |\n\n\n$factor\n:   \nA one_skim_df: 4 × 6\n\n| <!--/--> | skim_variable &lt;chr&gt; | n_missing &lt;int&gt; | complete_rate &lt;dbl&gt; | ordered &lt;lgl&gt; | n_unique &lt;int&gt; | top_counts &lt;chr&gt; |\n|---|---|---|---|---|---|---|\n| 1 | code  | 0 | 1 | FALSE |  7 | 100: 436, 100: 432, 100: 427, 100: 427 |\n| 2 | year  | 0 | 1 | FALSE |  2 | 201: 1511, 201: 1459                   |\n| 3 | month | 0 | 1 | FALSE | 12 | 3: 316, 1: 309, 4: 289, 12: 279        |\n| 4 | week  | 0 | 1 | FALSE |  7 | 2: 445, 4: 439, 1: 435, 5: 430         |\n\n\n$numeric\n:   \nA one_skim_df: 5 × 11\n\n| <!--/--> | skim_variable &lt;chr&gt; | n_missing &lt;int&gt; | complete_rate &lt;dbl&gt; | mean &lt;dbl&gt; | sd &lt;dbl&gt; | p0 &lt;dbl&gt; | p25 &lt;dbl&gt; | p50 &lt;dbl&gt; | p75 &lt;dbl&gt; | p100 &lt;dbl&gt; | hist &lt;chr&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1 | PM2.5 | 0 | 1 |  70.9060220 | 65.2264536 |   4.62500000 |  23.9154125 |  52.3405800 |   97.9219350 |  450.000000 | ▇▂▁▁▁ |\n| 2 | c_AOD | 0 | 1 |   0.6743631 |  0.6727058 |   0.02121083 |   0.2191133 |   0.4121369 |    0.8873678 |    3.562631 | ▇▂▁▁▁ |\n| 3 | TEMP  | 0 | 1 |  11.2723604 | 11.3385907 | -12.11607000 |   0.1136484 |  12.7025250 |   21.9393125 |   31.457240 | ▃▇▅▇▆ |\n| 4 | AP    | 0 | 1 | 995.0750959 | 18.2763409 | 939.35136800 | 989.3052500 | 998.3992768 | 1007.6977500 | 1028.826000 | ▁▁▃▇▃ |\n| 5 | RH    | 0 | 1 |  50.6296481 | 17.2962454 |   9.72653456 |  37.0803125 |  50.1837096 |   64.9926675 |   88.755170 | ▂▇▇▇▃ |\n\n\n\n\n",
            "text/latex": "\\begin{description}\n\\item[\\$character] A one\\_skim\\_df: 1 × 8\n\\begin{tabular}{r|llllllll}\n  & skim\\_variable & n\\_missing & complete\\_rate & min & max & empty & n\\_unique & whitespace\\\\\n  & <chr> & <int> & <dbl> & <int> & <int> & <int> & <int> & <int>\\\\\n\\hline\n\t1 & date & 0 & 1 & 9 & 10 & 0 & 501 & 0\\\\\n\\end{tabular}\n\n\\item[\\$factor] A one\\_skim\\_df: 4 × 6\n\\begin{tabular}{r|llllll}\n  & skim\\_variable & n\\_missing & complete\\_rate & ordered & n\\_unique & top\\_counts\\\\\n  & <chr> & <int> & <dbl> & <lgl> & <int> & <chr>\\\\\n\\hline\n\t1 & code  & 0 & 1 & FALSE &  7 & 100: 436, 100: 432, 100: 427, 100: 427\\\\\n\t2 & year  & 0 & 1 & FALSE &  2 & 201: 1511, 201: 1459                  \\\\\n\t3 & month & 0 & 1 & FALSE & 12 & 3: 316, 1: 309, 4: 289, 12: 279       \\\\\n\t4 & week  & 0 & 1 & FALSE &  7 & 2: 445, 4: 439, 1: 435, 5: 430        \\\\\n\\end{tabular}\n\n\\item[\\$numeric] A one\\_skim\\_df: 5 × 11\n\\begin{tabular}{r|lllllllllll}\n  & skim\\_variable & n\\_missing & complete\\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & hist\\\\\n  & <chr> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n\\hline\n\t1 & PM2.5 & 0 & 1 &  70.9060220 & 65.2264536 &   4.62500000 &  23.9154125 &  52.3405800 &   97.9219350 &  450.000000 & ▇▂▁▁▁\\\\\n\t2 & c\\_AOD & 0 & 1 &   0.6743631 &  0.6727058 &   0.02121083 &   0.2191133 &   0.4121369 &    0.8873678 &    3.562631 & ▇▂▁▁▁\\\\\n\t3 & TEMP  & 0 & 1 &  11.2723604 & 11.3385907 & -12.11607000 &   0.1136484 &  12.7025250 &   21.9393125 &   31.457240 & ▃▇▅▇▆\\\\\n\t4 & AP    & 0 & 1 & 995.0750959 & 18.2763409 & 939.35136800 & 989.3052500 & 998.3992768 & 1007.6977500 & 1028.826000 & ▁▁▃▇▃\\\\\n\t5 & RH    & 0 & 1 &  50.6296481 & 17.2962454 &   9.72653456 &  37.0803125 &  50.1837096 &   64.9926675 &   88.755170 & ▂▇▇▇▃\\\\\n\\end{tabular}\n\n\\end{description}\n",
            "text/plain": [
              "$character\n",
              "\n",
              "── Variable type: character ────────────────────────────────────────────────────\n",
              "  skim_variable n_missing complete_rate min max empty n_unique whitespace\n",
              "\u001b[90m1\u001b[39m date                  0             1   9  10     0      501          0\n",
              "\n",
              "$factor\n",
              "\n",
              "── Variable type: factor ───────────────────────────────────────────────────────\n",
              "  skim_variable n_missing complete_rate ordered n_unique top_counts             \n",
              "\u001b[90m1\u001b[39m code                  0             1 FALSE          7 100: 436, 100: 432, 10…\n",
              "\u001b[90m2\u001b[39m year                  0             1 FALSE          2 201: 1511, 201: 1459   \n",
              "\u001b[90m3\u001b[39m month                 0             1 FALSE         12 3: 316, 1: 309, 4: 289…\n",
              "\u001b[90m4\u001b[39m week                  0             1 FALSE          7 2: 445, 4: 439, 1: 435…\n",
              "\n",
              "$numeric\n",
              "\n",
              "── Variable type: numeric ──────────────────────────────────────────────────────\n",
              "  skim_…¹ n_mis…² compl…³    mean     sd       p0     p25     p50     p75   p100\n",
              "\u001b[90m1\u001b[39m PM2.5         0       1  70.9   65.2     4.62    23.9    52.3   9.79\u001b[90me\u001b[39m+1 4.5 \u001b[90me\u001b[39m2\n",
              "\u001b[90m2\u001b[39m c_AOD         0       1   0.674  0.673   0.021\u001b[4m2\u001b[24m   0.219   0.412 8.87\u001b[90me\u001b[39m\u001b[31m-1\u001b[39m 3.56\u001b[90me\u001b[39m0\n",
              "\u001b[90m3\u001b[39m TEMP          0       1  11.3   11.3   -\u001b[31m12\u001b[39m\u001b[31m.\u001b[39m\u001b[31m1\u001b[39m      0.114  12.7   2.19\u001b[90me\u001b[39m+1 3.15\u001b[90me\u001b[39m1\n",
              "\u001b[90m4\u001b[39m AP            0       1 995.    18.3   939.     989.    998.    1.01\u001b[90me\u001b[39m+3 1.03\u001b[90me\u001b[39m3\n",
              "\u001b[90m5\u001b[39m RH            0       1  50.6   17.3     9.73    37.1    50.2   6.50\u001b[90me\u001b[39m+1 8.88\u001b[90me\u001b[39m1\n",
              "\u001b[90m# … with 1 more variable: hist <chr>, and abbreviated variable names\u001b[39m\n",
              "\u001b[90m#   ¹​skim_variable, ²​n_missing, ³​complete_rate\u001b[39m\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dis_test <- skimr::skim(envir_example)\n",
        "\n",
        "print(head(dis_test,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOGzMv0bS6bu"
      },
      "source": [
        "## 1.2 Data split\n",
        "\n",
        "We randomly select 20% of the data as independent testing dataset, and the remainder were used as the training dataset.\n",
        "\n",
        "The split strategy is based on the size of your sample as well as your question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97tfiFloS_Iq"
      },
      "outputs": [],
      "source": [
        "set.seed(1234) # to achieve a repeatable resultsm\n",
        "\n",
        "size <-\n",
        "  caret::createDataPartition(y = envir_example$PM2.5,\n",
        "                             p = 0.8, list = FALSE)\n",
        "trainset <- envir_example[size, ]\n",
        "testset <- envir_example[-size, ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fsfC0myTEE0"
      },
      "source": [
        "Identify the dependence and independence variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAHawbUXTKZq"
      },
      "outputs": [],
      "source": [
        "y <- c(\"PM2.5\")\n",
        "x <- colnames(envir_example[-c(1, 6)]) # except \"date\" and \"PM2.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6VNxJe9TNFN"
      },
      "source": [
        "# Challenging 1\n",
        "\n",
        "Q: Estimate annual average NO2 in Sydney in 2005-2018\n",
        "\n",
        "* **Data prepare:** Download and load the Sydney NO2 data \"data_Sydney.csv\" in Google COlab by steps:\n",
        "\n",
        "Click file in the left side >  Upload to session storage > choose the files > OK\n",
        "\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1.  Setting the dependence (\"no2_annual\") and others as independent variables\n",
        "\n",
        "2.  Split 10% of data as testing data set\n",
        "\n",
        "```r\n",
        "#try it here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcSYWuKrhCCo"
      },
      "outputs": [],
      "source": [
        "data_Sydney <- read.csv(\"training_data.csv\")\n",
        "\n",
        "print(head(data_Sydney))\n",
        "\n",
        "# data clean\n",
        "\n",
        "data_Sydney$year <- as.factor(data_Sydney$year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otrFKmqkkxUy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dataset split\n",
        "set.seed(1234)\n",
        "size <-\n",
        "  caret::createDataPartition(y = data_Sydney$no2_annual,\n",
        "                             p = 0.9,\n",
        "                             list = FALSE)\n",
        "\n",
        "trainset_syd <- data_Sydney[size,]\n",
        "testset_syd <- data_Sydney[-size,]\n",
        "### Identify the dependence and independence variables\n",
        "\n",
        "dependence <- c(\"no2_annual\")\n",
        "independence <- colnames(data_Sydney[-c(1)]) # except \"no2_annual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4FbkYIWTX38"
      },
      "source": [
        "## 2. Establish base models\n",
        "\n",
        "## 2.1 Single base model training\n",
        "\n",
        "Q: How to select the best parameter for a single base model?\n",
        "\n",
        "\n",
        "We can set or adjust the parameters of a base model using 'tuningModel' function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUYj7zDqTdWe"
      },
      "outputs": [],
      "source": [
        "ranger <-\n",
        "  tuningModel(\n",
        "    basemodel  = 'SL.ranger',\n",
        "    params = list(num.trees = 100),\n",
        "    tune = list(mtry = c(1, 3, 7))\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAEBO3L0TgYV"
      },
      "source": [
        "Here we will train a Random Forest (RF) model with the specific parameters and using 5-fold Cross validation (CV) to assess the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri2beoeeTl_v"
      },
      "outputs": [],
      "source": [
        "# training the RF model with different parameters simultaneously\n",
        "\n",
        "start_time <- Sys.time()\n",
        "model1 <-\n",
        "  predictModel(\n",
        "    Y = trainset[, y],\n",
        "    X = trainset[, x],\n",
        "    base_model = c(ranger),\n",
        "    cvControl = list(V = 5)\n",
        "  )\n",
        "end_time <- Sys.time()\n",
        "end_time - start_time\n",
        "#print(model1$base_ensemble_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubewfl0mf8gR"
      },
      "source": [
        "The results show the weight, R2, and the root-mean-square error (RMSE) of each model. \"ranger_1\",\"ranger_2\",\"ranger_3\" note the Random Forest model with parameters mtry = 1,3,7 separately.\n",
        "\n",
        "The training results show that 'mtry = 7' could achieve a better RF model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvSOtCCLgAx3"
      },
      "source": [
        "## 2.2 Model prediction\n",
        "\n",
        "After training a base ML model, we can use it to estimate the independent testing dataset by using 'predict' function.\n",
        "\n",
        "**Note: 'predict()'function was recommended to limit its sources (namespace) to reduce the conflict with other R package such as stats::predict()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFpy97LCgEmx"
      },
      "outputs": [],
      "source": [
        "# compare the model performance in the independent testing dataset\n",
        "pred_model1 <- deeper::predict(object = model1, newX = testset[, x])\n",
        "\n",
        "base_model_output <- pred_model1$pre_base$library.predict\n",
        "\n",
        "print(head(base_model_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgiVjahEgRFY"
      },
      "outputs": [],
      "source": [
        "# calculate model performance in testing dataset\n",
        "print(apply(\n",
        "  X = pred_model1$pre_base$library.predict,\n",
        "  MARGIN = 2,\n",
        "  FUN = caret::R2,\n",
        "  obs = testset[, y]\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCDSj9VygTr4"
      },
      "source": [
        "After examine the model performance in an independent testing dataset, we may finally select the RF model with mtry = 7 as the best parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtaJPihagXPH"
      },
      "source": [
        "# Challenging 2\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1.  Establish a Random Forest model with parameter: mtry = 7 and others as default\n",
        "\n",
        "2.  Establish another base model 'xgboost' simultaneously by setting base_model = \"SL.xgboost\"\n",
        "\n",
        "```r\n",
        "#try it here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsQYUfp-oIhJ"
      },
      "outputs": [],
      "source": [
        "ranger_mtry_7 <-\n",
        "  tuningModel(basemodel  = 'SL.ranger',\n",
        "              params = list(mtry = 7))\n",
        "\n",
        "model_challenge2 <- predictModel(\n",
        "  Y = trainset_syd[, dependence],\n",
        "  X = trainset_syd[, independence],\n",
        "  base_model = c(ranger_mtry_7, \"SL.xgboost\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBbUIp7NgZTJ"
      },
      "source": [
        "## 2.3 Establish base model with parallel computing\n",
        "\n",
        "Considering the time-consuming of running several base models simultaneously, we can select using parallel computing to help improve the computational efficiency.\n",
        "\n",
        "We can identify the index in the cross validation(CV) to conduct the spatial (cluster) or temporal CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM_wUJ1YgU6d"
      },
      "outputs": [],
      "source": [
        "# there are 7 stations in the trainset\n",
        "unique(trainset$code) \n",
        "# Create a list with 7 (folds) elements (each element contains index of rows to be considered on each fold)\n",
        "\n",
        "## conduct the spatial CV\n",
        "indices <-\n",
        "  CAST::CreateSpacetimeFolds(trainset, spacevar = \"code\", k = 7)\n",
        "\n",
        "# Rows of validation set on each fold\n",
        "\n",
        "v_raw <- indices$indexOut\n",
        "names(v_raw) <- seq(1:7)\n",
        "\n",
        "start_time <- Sys.time()\n",
        "\n",
        "model2 <- predictModel_parallel(\n",
        "  Y = trainset[, y],\n",
        "  X = trainset[, x],\n",
        "  base_model = c(\"SL.ranger\"),\n",
        "  cvControl = list(V = length(v_raw), validRows = v_raw),\n",
        "  #number_cores = 2,\n",
        "  seed = 1234\n",
        ")\n",
        "end_time <- Sys.time()\n",
        "end_time - start_time\n",
        "## when number_cores is missing, it will indicate user to set one based on the operation system.\n",
        "\n",
        "#You have 2 cpu cores, How many cpu core you want to use:\n",
        "# type the number to continue the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu6X36kxg6fB"
      },
      "outputs": [],
      "source": [
        "# prediction for testing dataset\n",
        "pred_model2 <- deeper::predict(object = model2, newX = testset[, x])\n",
        "\n",
        "head(pred_model2$pre_base$library.predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYmrMcq5g-5T"
      },
      "source": [
        "## 3. Stacking meta models\n",
        "\n",
        "After assessing the performances of base models, we now can move forward to the DEML by stacking meta models on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNRbZTpRhARZ"
      },
      "outputs": [],
      "source": [
        "#Do include original feature\n",
        "#object do not include newX dataset\n",
        "\n",
        "## the training results\n",
        "\n",
        "cat(\"Training model results\")\n",
        "model3_stack <-\n",
        "  stack_ensemble(\n",
        "    object = model1,\n",
        "    meta_model = c(\"SL.ranger\", \"SL.xgboost\", \"SL.glm\"),\n",
        "    original_feature = FALSE,\n",
        "    #X = trainset[, x]\n",
        "  )\n",
        "#model3_stack$stack_ensemble_value\n",
        "\n",
        "# Independent testing results\n",
        "model3_DEML <-\n",
        "  deeper::predict(object = model3_stack, newX = testset[, x])\n",
        "cat(\"\\n\")\n",
        "cat(\"\\n\")\n",
        "cat(\"Independent testing results\")\n",
        "cat(\"\\n\")\n",
        "print(apply(\n",
        "  X = cbind(model3_DEML$pre_meta$library.predict,\n",
        "        model3_DEML$pre_meta$pred),\n",
        "  MARGIN = 2,\n",
        "  FUN = caret::R2,\n",
        "  obs = testset[, y]\n",
        "))\n",
        "\n",
        "print(apply(\n",
        "  X = cbind(model3_DEML$pre_meta$library.predict,\n",
        "        model3_DEML$pre_meta$pred),\n",
        "  MARGIN = 2,\n",
        "  FUN = caret::RMSE,\n",
        "  obs = testset[, y]\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIDpYqaKhvOB"
      },
      "source": [
        "## 3.1 Stacked meta models from scratch\n",
        "\n",
        "We can create DEML directly by setting the base models and meta models. But considering the unknown impact of the underlying model and computation time, this is not recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFf0qgdQhyij"
      },
      "outputs": [],
      "source": [
        "model4_stack <-\n",
        "  stack_ensemble.fit(\n",
        "    Y = trainset[, y],\n",
        "    X = trainset[, x],\n",
        "    base_model = c(\"SL.ranger\", \"SL.xgboost\"),\n",
        "    meta_model = c(\"SL.glm\"),\n",
        "    original_feature = FALSE\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM3aFtvYh0ka"
      },
      "source": [
        "## 3.2 Stacked meta models with paralleling computing\n",
        "\n",
        "We also accelerate the calculation with paralleling computing in DEML. \n",
        "\n",
        "Several key points are worthy to note:\n",
        "\n",
        "- If the base model used parallel computing, the meta-model also needs to be parallel.\n",
        "\n",
        "- When setting a specific CV index, please be consistent in meta-model training.\n",
        "\n",
        "- Do not use all of your computation cores to do paralleling. Leave at least one for your operating system.\n",
        "\n",
        "- 'Original_feature' is optional. It may generally improve your model performance but increase the computational complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCTj11L6h7op"
      },
      "outputs": [],
      "source": [
        "#Do not include original feature\n",
        "\n",
        "start_time <- Sys.time()\n",
        "model5_stack <-\n",
        "  stack_ensemble_parallel(\n",
        "    object = model2,\n",
        "    Y = trainset[, y],\n",
        "    meta_model = c(\"SL.glm\",\"SL.ranger\"),\n",
        "    original_feature = FALSE,\n",
        "    cvControl = list(V = length(v_raw), validRows = v_raw),\n",
        "    number_cores = 2\n",
        "  )\n",
        "end_time <- Sys.time()\n",
        "end_time - start_time\n",
        "# the training results\n",
        "\n",
        "# the testing results\n",
        "pred_model5_stack <-\n",
        "  deeper::predict(object = model5_stack, newX = testset[, x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njAU-XvxjYDo"
      },
      "source": [
        "# Challenging 3\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Stacking meta models  using RF and Xgboost (with original features) in Sydeny data to conduct DEML model\n",
        "\n",
        "2. Achieving the final DEML model performance R2 in testing dataset\n",
        "\n",
        "```r\n",
        "#try it here\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs0iwwWYoTDa"
      },
      "outputs": [],
      "source": [
        "# training the DEML model\n",
        "model_challenging3 <- stack_ensemble(\n",
        "  object = model_challenge2,\n",
        "  Y = trainset_syd[, dependence],\n",
        "  X = trainset_syd[, independence],\n",
        "  meta_model = c(\"SL.ranger\", \"SL.xgboost\"),\n",
        "  original_feature = TRUE\n",
        ")\n",
        "\n",
        "# model performance for test dataset\n",
        "pred_DEML <-\n",
        "  deeper::predict(object = model_challenging3, newX = testset_syd[, independence])\n",
        "\n",
        "print(apply(\n",
        "  X = pred_DEML$pre_meta$pred,\n",
        "  MARGIN = 2,\n",
        "  FUN = caret::R2,\n",
        "  obs = testset_syd[, dependence]\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vplGLasVjdkh"
      },
      "source": [
        "## 3.3 Using DEML to estimate the grid cell data\n",
        "\n",
        "The same as the base model prediction, we use 'deeper::predict()' function to use the established DEML model to estimate the grid cell air pollutant.\n",
        "\n",
        "**Note: Please make sure all grid input data should keep the same data structure and variable types with the training data. Using 'attr.all.equal' to test the similarity.**\n",
        "\n",
        "**Tips: Using 'tidyverse::bind_rows' to combine the training data and grid data first. And then splitting the grid data for prediction can always help to keep the same structure with training data.**\n",
        "\n",
        "# Challenging 4\n",
        "Download and load the Sydney gridded cell NO2 data \"predictor_scaled_no2_gsyd_10km_2005_2018_WY3.csv\" in Google COlab by steps:\n",
        "\n",
        "Click file in the left side> Upload to session storage > choose the file > OK\n",
        "\n",
        "\n",
        "Task:\n",
        "\n",
        "To estimate 10km grid cell yearly NO2 in Sydney.\n",
        "\n",
        "```r\n",
        "#try it here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbHsS9uommYV"
      },
      "outputs": [],
      "source": [
        "# load the grid cell dataset\n",
        "\n",
        "predictor_scaled_no2_gsyd_10km_2005_2018 <- read.csv(file = \"new_data.csv\")\n",
        "\n",
        "predictor_scaled_no2_gsyd_10km_2005_2018$year <- as.factor(predictor_scaled_no2_gsyd_10km_2005_2018$year)\n",
        "\n",
        "# estimate grid cell NO2 using the model created in Challenging 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred_DEML_grid <-\n",
        "  deeper::predict(object = model_challenging3, newX = predictor_scaled_no2_gsyd_10km_2005_2018)\n",
        "\n",
        "# get the final DEML prediction\n",
        "DEML_model_output <- pred_DEML_grid$pre_meta$pred\n",
        "\n",
        "print(head(DEML_model_output))"
      ],
      "metadata": {
        "id": "wZS7CATSEGPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5PtQPd_jlWo"
      },
      "source": [
        "## 4. Plot the results\n",
        "\n",
        "We can finally have the scatter plot using 'assess.plot' function in deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90zj3H0tjoiD"
      },
      "outputs": [],
      "source": [
        "plot_DEML <-\n",
        "  assess.plot(pre = model3_DEML$pre_meta$pred, obs = testset[, y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eee9EO6sx3L4"
      },
      "outputs": [],
      "source": [
        "#install.packages(\"hexbin\")\n",
        "#library(hexbin)\n",
        "plot_DEML$plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJqJSbESLxjs"
      },
      "source": [
        "# **Demostrate the deeper GUI by Liam**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FYco_VmMLQo"
      },
      "source": [
        "Please install a deeper GUI software first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slYmv6gwjq2b"
      },
      "source": [
        "# Citation\n",
        "\n",
        "Wenhua Yu, Shanshan Li, Tingting Ye,Rongbin Xu, Jiangning Song, Yuming Guo (2022) Deep ensemble machine learning framework for the estimation of PM2.5 concentrations,Environmental health perspectives: [https://doi.org/10.1289/EHP9752](https://doi.org/10.1289/EHP9752)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOsV6erOdzySbQM9uFT6lH3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}